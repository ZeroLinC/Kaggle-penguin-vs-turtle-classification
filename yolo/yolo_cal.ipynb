{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean distance: 0.12460187106854954\n",
      "Standard Deviation of distance: 0.12068045885788577\n",
      "Mean IoU: 0.7250951305519128\n",
      "Standard Deviation of IoU: 0.24165147425530464\n"
     ]
    }
   ],
   "source": [
    "def compute_center(box):\n",
    "    return ((box[0] + box[2]) / 2, (box[1] + box[3]) / 2)\n",
    "\n",
    "def compute_iou(boxA, boxB):\n",
    "    xA = max(boxA[0], boxB[0])\n",
    "    yA = max(boxA[1], boxB[1])\n",
    "    xB = min(boxA[2], boxB[2])\n",
    "    yB = min(boxA[3], boxB[3])\n",
    "\n",
    "    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n",
    "\n",
    "    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n",
    "    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n",
    "\n",
    "    iou = interArea / float(boxAArea + boxBArea - interArea)\n",
    "    return iou\n",
    "\n",
    "predicted_path = \"runs/detect/predict1/labels\"\n",
    "true_path = \"labels/valid\"\n",
    "\n",
    "distances = []\n",
    "ious = []\n",
    "pred_labels = []\n",
    "true_labels = []\n",
    "\n",
    "for filename in os.listdir(predicted_path):\n",
    "    with open(os.path.join(predicted_path, filename), 'r') as f:\n",
    "        preds_data = [line.split() for line in f.readlines()]\n",
    "        preds = [list(map(float, data[1:])) for data in preds_data]\n",
    "        pred_label = [int(data[0]) for data in preds_data]\n",
    "\n",
    "    with open(os.path.join(true_path, filename), 'r') as f:\n",
    "        true_data = f.readline().split()\n",
    "        true = list(map(float, true_data[1:]))\n",
    "        true_label = int(true_data[0])\n",
    "\n",
    "    center_true = compute_center(true)\n",
    "    min_distance = float('inf')\n",
    "    best_iou = 0\n",
    "    best_pred_label = None\n",
    "    \n",
    "    for i, pred in enumerate(preds):\n",
    "        center_pred = compute_center(pred)\n",
    "\n",
    "        distance = np.linalg.norm(np.array(center_pred) - np.array(center_true))\n",
    "        if distance < min_distance:\n",
    "            min_distance = distance\n",
    "            best_iou = compute_iou(pred, true)\n",
    "            best_pred_label = pred_label[i]\n",
    "\n",
    "    distances.append(min_distance)\n",
    "    ious.append(best_iou)\n",
    "    pred_labels.append(best_pred_label)\n",
    "    true_labels.append(true_label)\n",
    "\n",
    "print(f\"Mean distance: {np.mean(distances)}\")\n",
    "print(f\"Standard Deviation of distance: {np.std(distances)}\")\n",
    "print(f\"Mean IoU: {np.mean(ious)}\")\n",
    "print(f\"Standard Deviation of IoU: {np.std(ious)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The percentage of original images which has detection is: 83.33%\n"
     ]
    }
   ],
   "source": [
    "num_predicted_imgs = len(os.listdir(predicted_path))\n",
    "num_origin_imgs = len(os.listdir(true_path))\n",
    "ratio = round((num_predicted_imgs/num_origin_imgs)*100,2)\n",
    "print(f\"The percentage of original images which has detection is: {ratio}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[27  5]\n",
      " [ 3 25]]\n",
      "Accuracy: 86.67%\n",
      "Precision: 86.67%\n",
      "Recall: 86.83%\n",
      "F1-Score: 86.65%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Assuming `pred_labels` are your predicted labels and `true_labels` are the ground truth\n",
    "conf_matrix = confusion_matrix(true_labels, pred_labels)\n",
    "accuracy = accuracy_score(true_labels, pred_labels)\n",
    "precision = precision_score(true_labels, pred_labels, average='macro')  # 'macro' for binary problem\n",
    "recall = recall_score(true_labels, pred_labels, average='macro')\n",
    "f1 = f1_score(true_labels, pred_labels, average='macro')\n",
    "\n",
    "print(f\"Confusion Matrix:\\n{conf_matrix}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(f\"Precision: {precision:.2%}\")\n",
    "print(f\"Recall: {recall:.2%}\")\n",
    "print(f\"F1-Score: {f1:.2%}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "comp9517",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
